
## Table of Contents

1. [Project Overview](#project-overview)
2. [Directory Structure](#directory-structure)
3. [Data Folders](#data-folders)
4. [Source Code](#source-code)
5. [Pipeline Workflow](#pipeline-workflow)
6. [Technical Specifications](#technical-specifications)
7. [Results and Evaluation](#results-and-evaluation)
8. [Usage Guide](#usage-guide)

---

## Project Overview

This project implements a deep learning-based speaker recognition system capable of identifying 20 different speakers from isolated name utterances. The system includes:

- **MFCC Feature Extraction**: 120-dimensional features (40 MFCC + Δ + ΔΔ)
- **CNN Architecture**: 2D Convolutional Neural Network for acoustic modeling
- **Noise Compensation**: Two strategies (baseline and matched training)
- **Evaluation**: Performance assessment across clean and noisy conditions (SNR 20/10/0 dB)
- **Testing Interface**: Command-line tool for testing with new audio recordings

**Key Capabilities**:
- Speaker-dependent recognition (trained on single speaker's recordings)
- Isolated word recognition (20 names vocabulary)
- Noise robustness testing with varying SNR levels
- Real-time prediction with confidence scores

---

## Directory Structure

```
TestingAOu-locase/
├── Audio/                    # Training audio dataset (800 files, 40 per speaker)
├── Audio_noise/              # Noise samples for augmentation
├── Audio_noisy/              # Generated noisy audio (SNR 20/10/0 dB)
├── Audio_test/               # Test recordings (user's voice)
├── features/                 # Extracted MFCC features (NPZ + JSON)
├── models/                   # Trained Keras models
├── results/                  # Evaluation outputs (reports, plots, confusion matrices)
├── tasks/                    # Main pipeline scripts
├── helpers/                  # Utility modules
├── docs/                     # Documentation and instructions
├── run_project.py            # Master pipeline runner
├── reset_project.py          # Clean all generated outputs
├── TESTING_GUIDE.md          # How to test with new recordings
└── PROJECT_DOCUMENTATION.md  # This file
```

---

## Data Folders

### `Audio/` - Training Dataset
**Purpose**: Contains the primary training dataset  
**Structure**: 
- 800 WAV files total (40 files × 20 speakers)
- Naming convention: `speakername###.wav` (e.g., `ahmed000.wav`)
- Audio format: 16kHz, mono, isolated name utterances

**How it works**:
1. Speaker names are extracted from filenames using pattern matching
2. Each file contains a single person speaking one name
3. Files are processed in alphabetical order for reproducibility

**Speakers** (20 total):
ahmed, amber, charlie, christopher, dominic, emad, emma, hannah, imogen, jess, josh, joshua, kailong, kira, manwel, mateusz, ngozi, riley, sivaprasath, zack

---

### `Audio_noise/` - Noise Samples
**Purpose**: Background noise recordings for data augmentation  
**Structure**: WAV files containing various noise types (factory, babble, etc.)

**How it works**:
- Noise samples are mixed with clean speech at specified SNR levels
- Used to create realistic noisy conditions for training and testing
- Enables evaluation of noise robustness

---

### `Audio_noisy/` - Generated Noisy Dataset
**Purpose**: Automatically generated noisy versions of training data  
**Structure**:
```
Audio_noisy/
├── mixing_stats.json         # Metadata about mixing process
├── SNR0/                     # 0 dB SNR (very noisy)
├── SNR10/                    # 10 dB SNR (moderate noise)
└── SNR20/                    # 20 dB SNR (light noise)
```

**How it works**:
1. Script: `tasks/task4_create_noise.py`
2. For each clean audio file:
   - Load clean speech signal
   - Load random noise segment (same duration)
   - Mix at target SNR using power-based scaling
   - Save to appropriate SNR folder
3. SNR calculation: `SNR = 10 * log10(P_signal / P_noise)`

**Generated by**: Task 4 (Noise Creation)  
**Used by**: Task 4 (Noise Compensation Training)

---

### `Audio_test/` - Test Recordings
**Purpose**: User's own voice recordings for testing the trained model  
**Structure**: 
- WAV files named: `speakername_###.wav`
- Example: `ahmed_000.wav`, `charlie_001.wav`

**How it works**:
1. User records themselves saying names from the vocabulary
2. Files are placed in this folder with proper naming
3. `demo_test.py` extracts features and runs prediction
4. Model predicts which training speaker the voice sounds most similar to

**Note**: These files are NOT used for training, only for testing/demonstration

---

### `features/` - Extracted Features
**Purpose**: Preprocessed MFCC features ready for model training  
**Files**:
- `features.npz` - Clean audio features (X, y arrays)
- `features.json` - Metadata (labels, shape, config)
- `noisy_snr0.npz`, `noisy_snr0.json` - 0 dB SNR features
- `noisy_snr10.npz`, `noisy_snr10.json` - 10 dB SNR features
- `noisy_snr20.npz`, `noisy_snr20.json` - 20 dB SNR features

**NPZ Structure**:
```python
{
    'X': np.ndarray,  # Shape: (N, 120, max_frames, 1)
                      # N = number of samples
                      # 120 = MFCC dimensions (40 + Δ + ΔΔ)
                      # max_frames = 99 (dynamic from dataset)
                      # 1 = channel dimension for CNN
    'y': np.ndarray   # Shape: (N,) - Speaker labels as strings
}
```

**JSON Metadata**:
```json
{
    "n_mfcc": 120,
    "max_frames": 99,
    "num_samples": 800,
    "num_classes": 20,
    "labels": ["ahmed", "amber", ...],
    "label_to_idx": {"ahmed": 0, "amber": 1, ...},
    "sample_rate": 16000,
    "win_ms": 25,
    "hop_ms": 10,
    "n_fft": 512
}
```

**How features are extracted** (see `helpers/feature_utils.py`):
1. **Load audio**: Resample to 16kHz mono using `soundfile` and `librosa`
2. **Framing**: 25ms windows with 10ms hop using Hamming window
3. **Mel Filterbank**: 40 triangular filters on mel scale (0-8000 Hz)
4. **MFCCs**: DCT on log mel energies → 40 coefficients
5. **Deltas**: First (Δ) and second (ΔΔ) derivatives → 120 total features
6. **Padding**: Pad/truncate to max_frames for uniform shape
7. **Reshape**: Transpose to (120, max_frames) and add channel dimension

**Generated by**: Task 2 (Feature Extraction)  
**Used by**: Tasks 3, 4, 5 (Training and Evaluation)

---

### `models/` - Trained Models
**Purpose**: Saved Keras models ready for inference  
**Files**:
- `task3_baseline.keras` - Basic CNN trained on clean audio only
- `task4_baseline.keras` - Baseline noise compensation (clean training, noisy testing)
- `task4_matched.keras` - Matched training (clean + noisy conditions)

**Model Architecture** (Baseline CNN):
```
Input: (120, 99, 1)
  ↓
Conv2D(64 filters, 3×3 kernel, ReLU)
  ↓
MaxPooling2D(3×3 pool)
  ↓
Flatten
  ↓
Dense(256, ReLU)
  ↓
Dense(20, Softmax) ← Output: 20 speaker classes
```

**Training Configuration**:
- Optimizer: Adam (learning rate = 1e-3)
- Loss: Categorical crossentropy
- Batch size: 32
- Max epochs: 20
- Early stopping: Patience 15 on validation loss
- Data split: 80% train / 10% val / 10% test

**Model Differences**:
1. **Task 3 Baseline**: Trained on clean audio, evaluated on test set
2. **Task 4 Baseline**: Trained on clean, evaluated on ALL data (clean + noisy)
3. **Task 4 Matched**: Trained on clean + all noisy conditions (4× data), evaluated separately

**Generated by**: Tasks 3 and 4  
**Used by**: Task 5 (Evaluation), demo_test.py (Testing)

---

### `results/` - Evaluation Outputs
**Purpose**: Stores all evaluation metrics, plots, and reports  
**Structure**:
```
results/
├── task3/
│   ├── confusion_matrix.png       # Visual confusion matrix
│   ├── training_history.png       # Loss/accuracy curves
│   └── model_summary.txt          # Architecture summary
└── task4/
    ├── baseline/                  # Baseline strategy results
    │   ├── confusion_clean.png
    │   ├── confusion_SNR0.png
    │   ├── confusion_SNR10.png
    │   ├── confusion_SNR20.png
    │   ├── classification_report_clean.txt
    │   ├── classification_report_SNR0.txt
    │   ├── classification_report_SNR10.txt
    │   ├── classification_report_SNR20.txt
    │   ├── training_history_baseline.png
    │   └── baseline_best.keras     # Best model checkpoint
    └── matched/                   # Matched training results
        ├── confusion_clean_matched.png
        ├── confusion_SNR0_matched.png
        ├── confusion_SNR10_matched.png
        ├── confusion_SNR20_matched.png
        ├── classification_report_clean_matched.txt
        ├── classification_report_SNR0_matched.txt
        ├── classification_report_SNR10_matched.txt
        ├── classification_report_SNR20_matched.txt
        ├── training_history_matched.png
        └── matched_best.keras
```

**Confusion Matrix** (PNG files):
- 20×20 heatmap showing true vs predicted labels
- Diagonal values = correct predictions
- Off-diagonal = confusions between speakers
- **Important**: Shows counts for ALL 800 samples (40 per speaker) following lab4 methodology

**Classification Report** (TXT files):
- Per-speaker precision, recall, F1-score
- Support = 40 samples per speaker
- Overall accuracy and macro/weighted averages

**Training History** (PNG files):
- Two subplots: Loss and Accuracy over epochs
- Training vs Validation curves
- Useful for diagnosing overfitting/underfitting

**Generated by**: Tasks 3 and 4  
**Used for**: Performance analysis and presentation

---

## Source Code

### `tasks/` - Main Pipeline Scripts

#### `task2.py` - Feature Extraction (Clean Audio)
**Purpose**: Extract MFCC features from clean training audio  
**Input**: `Audio/*.wav` (800 files)  
**Output**: `features/features.npz`, `features/features.json`

**How it works**:
1. Scan `Audio/` folder for all .wav files
2. For each file:
   - Extract speaker name from filename (e.g., "ahmed000.wav" → "ahmed")
   - Load audio and resample to 16kHz
   - Frame signal (25ms windows, 10ms hop, Hamming)
   - Extract 40 MFCCs from mel filterbank
   - Add delta (Δ) and delta-delta (ΔΔ) → 120 features
   - Store features and label
3. Determine max_frames from dataset (99 frames)
4. Pad all features to uniform shape (120, 99)
5. Transpose and add channel dimension → (800, 120, 99, 1)
6. Save features (NPZ) and metadata (JSON)

**Key Function**: `extract_name_from_filename()`
```python
def extract_name_from_filename(filename):
    """Extract speaker name from filename pattern speakername###.wav"""
    stem = Path(filename).stem
    name = ''.join([c for c in stem if c.isalpha()]).lower()
    return name
```

---

#### `task2_noisy.py` - Feature Extraction (Noisy Audio)
**Purpose**: Extract MFCC features from noisy audio  
**Input**: `Audio_noisy/SNR{0,10,20}/*.wav`  
**Output**: `features/noisy_snr{0,10,20}.npz` and `.json`

**How it works**:
- Same process as task2.py, but applied to noisy audio folders
- Maintains same max_frames (99) for compatibility
- Generates separate feature files for each SNR level

---

#### `task3.py` - Baseline CNN Training
**Purpose**: Train baseline CNN on clean audio  
**Input**: `features/features.npz`  
**Output**: 
- `models/task3_baseline.keras`
- `results/task3/confusion_matrix.png`
- `results/task3/training_history.png`
- `results/task3/model_summary.txt`

**How it works**:
1. Load features from NPZ file
2. Split data: 80% train / 10% val / 10% test (stratified by speaker)
3. Normalize features using training set statistics:
   ```python
   X_norm = (X - train_mean) / (train_std + 1e-8)
   ```
4. Build CNN model (see architecture above)
5. Train for 20 epochs with early stopping
6. Evaluate on ALL data (800 samples) following lab4 methodology
7. Generate confusion matrix and classification report
8. Save model and visualizations

**Key Design Choice**: Evaluates on full dataset (not just test split) to show performance across all samples, matching lab4 requirements.

---

#### `task4_create_noise.py` - Noisy Data Generation
**Purpose**: Create noisy versions of clean audio at different SNR levels  
**Input**: 
- `Audio/*.wav` (clean speech)
- `Audio_noise/*.wav` (noise samples)
**Output**: `Audio_noisy/SNR{0,10,20}/*.wav`

**How it works**:
1. For each clean audio file:
   - Load clean signal: `x_clean`
   - Calculate signal power: `P_signal = mean(x_clean²)`
2. For each target SNR (0, 10, 20 dB):
   - Load random noise segment (same duration as speech)
   - Calculate required noise power: `P_noise = P_signal / (10^(SNR/10))`
   - Scale noise to target power
   - Mix: `x_noisy = x_clean + noise_scaled`
   - Save to `Audio_noisy/SNR{snr}/filename.wav`
3. Save mixing statistics to JSON

**SNR Formula**:
```python
SNR_dB = 10 * log10(P_signal / P_noise)

# Solving for noise scaling:
P_noise_target = P_signal / (10 ** (SNR_dB / 10))
scaling_factor = sqrt(P_noise_target / P_noise_current)
noise_scaled = noise * scaling_factor
```

---

#### `task4_train.py` - Noise Compensation Training
**Purpose**: Train two noise compensation strategies  
**Input**: All feature files (clean + noisy)  
**Output**: Models and evaluation results for both strategies

**Strategy 1: Baseline**
- Train on clean audio only (640 samples)
- Evaluate on ALL clean data (800 samples)
- Evaluate on ALL noisy data for each SNR (800 samples each)
- Shows how clean-only training degrades with noise

**Strategy 2: Matched Training**
- Combine all datasets: clean + SNR20 + SNR10 + SNR0 = 3200 samples
- Split combined data: 80/10/10
- Train on mixed conditions (2560 samples)
- Evaluate each condition separately (800 samples per condition)
- Shows benefit of training on noise-matched data

**How it works**:
1. Load all feature files (clean, SNR20, SNR10, SNR0)
2. For baseline:
   - Split clean data only
   - Train CNN on clean training set
   - Normalize ALL data using clean training statistics
   - Evaluate on all 800 clean samples
   - Evaluate on all 800 noisy samples per SNR
3. For matched:
   - Concatenate all datasets (3200 samples)
   - Split combined data
   - Train CNN on mixed training set
   - Evaluate each condition separately using same normalization
4. Generate confusion matrices and reports for all conditions
5. Compare performance across strategies

**Key Insight**: Matched training should show better performance on noisy conditions due to exposure during training.

---

#### `task5.py` - Final Evaluation
**Purpose**: Summarize and compare all models  
**Input**: All trained models and feature files  
**Output**: Accuracy table comparing models across conditions

**How it works**:
1. Load all models (task3, task4_baseline, task4_matched)
2. Load all feature files (clean, SNR20, SNR10, SNR0)
3. For each model × condition combination:
   - Run inference
   - Calculate accuracy
4. Print formatted table:
   ```
   Model                Clean    SNR20    SNR10    SNR0
   Task3 Baseline       95.0%    78.8%    32.5%   16.2%
   Task4 Baseline       91.2%    75.0%    38.8%   16.2%
   Task4 Matched       100.0%   100.0%   100.0%   95.0%
   ```

---

#### `demo_test.py` - Testing Interface
**Purpose**: Test trained model with new audio recordings  
**Input**: User's test recordings in `Audio_test/`  
**Output**: Predictions with confidence scores

**How it works**:
1. Load trained model and metadata (label mappings, max_frames)
2. For each test file:
   - Extract MFCC features (same pipeline as task2)
   - Normalize using training statistics
   - Run model.predict()
   - Get predicted class and confidence
3. Display results:
   - Single file: Top-N predictions with probabilities
   - Batch: Table of predictions with accuracy

**Command-line interface**:
```bash
# Single file
python demo_test.py --audio Audio_test/ahmed_000.wav --true-label ahmed --top-n 10

# Batch test
python demo_test.py --pattern "Audio_test/*.wav" --true-label ahmed

# Different model
python demo_test.py --audio Audio_test/ahmed_000.wav --model models/task3_baseline.keras
```

**Key Functions**:
- `extract_features_from_audio()`: Feature extraction pipeline
- `predict_speaker()`: Run inference and get predictions
- `test_single_file()`: Test one file with detailed output
- `test_batch_files()`: Test multiple files with summary table

---

### `helpers/` - Utility Modules

#### `feature_utils.py` - Feature Extraction Functions
**Purpose**: Core signal processing and feature extraction  
**Key Functions**:

**`load_wav(path, target_sr=16000)`**
- Load WAV file using soundfile
- Convert to mono if stereo
- Resample to target sample rate using librosa
- Return: 1D numpy array (float32, normalized)

**`frame_signal(x, sr, win_ms=25, hop_ms=10, window="hamming")`**
- Frame signal into overlapping windows
- Convert ms to samples: `frame_size = int(win_ms * sr / 1000)`
- Calculate number of frames needed
- Zero-pad signal if necessary
- Apply window function (Hamming)
- Return: 2D array (num_frames, frame_size)

**`mel_filterbank(sr, n_fft=512, n_mels=40)`**
- Create triangular mel-scale filterbank
- Frequency range: 0 to sr/2 (0-8000 Hz for 16kHz)
- Convert Hz to mel: `mel = 2595 * log10(1 + f/700)`
- Create n_mels equally-spaced points on mel scale
- Convert back to Hz
- Create triangular filters between mel points
- Return: Filter matrix (n_mels, n_fft//2 + 1)

**`mfcc_from_frames(frames, sr, n_fft=512, keep_mfcc=40)`**
- Compute power spectrum for each frame
- Apply mel filterbank to get mel energies
- Take logarithm: `log_mel = log(mel_energies + epsilon)`
- Apply DCT (Discrete Cosine Transform)
- Keep first 40 coefficients
- Return: (num_frames, 40)

**`add_deltas(mfcc, order=2, width=2)`**
- Compute first derivative (Δ):
  ```python
  delta = (mfcc[t+1] - mfcc[t-1]) / 2
  ```
- Compute second derivative (ΔΔ):
  ```python
  delta_delta = (delta[t+1] - delta[t-1]) / 2
  ```
- Concatenate: [MFCC, Δ, ΔΔ]
- Return: (num_frames, 120)

**`pad_features(feat_2d, max_frames)`**
- Pad with zeros if too short
- Truncate if too long
- Ensure uniform shape for CNN input

**`normalize_features(X_train, X_val, X_test)`**
- Calculate mean and std from training set only
- Apply same normalization to all sets
- Prevent data leakage from val/test sets

**`save_features(X, y, metadata, out_path)`**
- Save features as NPZ (compressed numpy)
- Save metadata as JSON
- Include configuration for reproducibility

**`load_features(base_path)`**
- Load NPZ and JSON
- Return X, y, metadata tuple

---

#### `noise_utils.py` - Noise Mixing Functions
**Purpose**: Add noise to clean audio at specific SNR levels  
**Key Functions**:

**`add_noise_at_snr(signal, noise, target_snr_db)`**
- Calculate signal power: `P_signal = mean(signal²)`
- Calculate noise power: `P_noise = mean(noise²)`
- Compute target noise power from SNR
- Scale noise to match target power
- Mix: `noisy = signal + noise_scaled`
- Return: noisy signal

**`mix_audio_folder(clean_dir, noise_dir, output_dir, snr_db)`**
- Iterate through clean files
- For each file:
  - Load clean audio
  - Load random noise segment (same duration)
  - Mix at target SNR
  - Save to output directory
- Track statistics (SNR distribution, file count)

---

#### `check_speakers.py` - Dataset Validation
**Purpose**: Verify dataset integrity and speaker distribution  
**Functions**:
- Count files per speaker
- Check for naming anomalies
- Verify expected 40 files per speaker
- Report any missing or extra files

---

#### `find_duplicates.py` - Duplicate Detection
**Purpose**: Find duplicate audio files in dataset  
**How it works**:
- Calculate audio hash for each file
- Group files by hash
- Report any duplicates
- Useful for ensuring data quality

---

#### `verify_features.py` - Feature Validation
**Purpose**: Verify feature extraction correctness  
**Checks**:
- Feature shape matches expected dimensions
- No NaN or Inf values
- Label counts match file counts
- Metadata consistency

---

### `docs/` - Documentation

#### `lab4.txt` - Lab Instructions
Original lab exercise showing:
- How to build DNNs with TensorFlow/Keras
- Feature extraction from speech
- Training/validation/test splits
- **Important**: Shows evaluation on full dataset (not just test split)

#### `Project-instructions.txt` - Coursework Specification
Detailed assignment requirements:
- Speech data collection (20+ samples per name)
- Feature extraction (MFCCs with derivatives)
- Acoustic modeling (DNNs with 2D conv layers)
- Noise compensation (spectral subtraction or matched training)
- Testing and evaluation methodology

#### `README.md` - Quick Start Guide
Brief overview of project structure and how to run

---

## Pipeline Workflow

### Complete Execution Order

```
1. run_project.py (Master controller)
   ↓
2. Task 2: Feature Extraction (Clean)
   Input:  Audio/*.wav
   Output: features/features.npz, features.json
   ↓
3. Task 2 (Noisy): Create Noisy Data
   Input:  Audio/*.wav, Audio_noise/*.wav
   Output: Audio_noisy/SNR{0,10,20}/*.wav
   ↓
4. Task 2 (Noisy): Extract Noisy Features
   Input:  Audio_noisy/SNR{0,10,20}/*.wav
   Output: features/noisy_snr{0,10,20}.npz
   ↓
5. Task 3: Train Baseline CNN
   Input:  features/features.npz
   Output: models/task3_baseline.keras, results/task3/*
   ↓
6. Task 4: Noise Compensation Training
   Input:  All feature files
   Output: models/task4_{baseline,matched}.keras, results/task4/*
   ↓
7. Task 5: Final Evaluation
   Input:  All models and features
   Output: Accuracy comparison table
   ↓
8. (Optional) Testing with demo_test.py
   Input:  Audio_test/*.wav, trained models
   Output: Predictions with confidence scores
```

### Data Flow Diagram

```
RAW AUDIO (800 files)
        ↓
    [Task 2]
        ↓
MFCC FEATURES (120 × 99)
        ↓
   [Split 80/10/10]
        ↓
TRAIN (640) / VAL (80) / TEST (80)
        ↓
    [Task 3]
        ↓
BASELINE MODEL (Clean only)
        ↓
   [Evaluation]
        ↓
CONFUSION MATRIX (800 samples)


NOISE SAMPLES + CLEAN AUDIO
        ↓
  [Task 4 Create]
        ↓
NOISY AUDIO (3 × 800 files)
        ↓
  [Task 2 Noisy]
        ↓
NOISY FEATURES (3 × 800)
        ↓
  [Task 4 Train]
        ↓
BASELINE + MATCHED MODELS
        ↓
   [Evaluation]
        ↓
CONFUSION MATRICES (4 conditions × 2 strategies)
```

---

## Technical Specifications

### Audio Processing
- **Sample Rate**: 16 kHz (mono, float32)
- **Frame Length**: 25 ms (~400 samples)
- **Hop Length**: 10 ms (~160 samples)
- **Window**: Hamming
- **FFT Size**: 512
- **Frequency Range**: 0-8000 Hz

### Feature Extraction
- **Mel Filters**: 40 triangular filters
- **MFCC Coefficients**: 40
- **Deltas**: Δ + ΔΔ (first and second derivatives)
- **Total Features**: 120 dimensions
- **Temporal Length**: 99 frames (dynamic from dataset)
- **Final Shape**: (120, 99, 1)

### Neural Network
- **Architecture**: 2D Convolutional Neural Network
- **Input**: (120, 99, 1)
- **Conv2D**: 64 filters, 3×3 kernel, ReLU activation
- **MaxPooling**: 3×3 pool size
- **Dense**: 256 neurons, ReLU activation
- **Output**: 20 classes, Softmax activation
- **Parameters**: ~21.6M trainable parameters

### Training
- **Optimizer**: Adam (learning rate = 1e-3)
- **Loss**: Categorical Crossentropy
- **Metrics**: Accuracy
- **Batch Size**: 32
- **Epochs**: 20 (with early stopping)
- **Early Stopping**: Patience 15 on validation loss
- **Data Split**: 80% train / 10% validation / 10% test
- **Callbacks**: ModelCheckpoint (save best model)

### Noise Compensation
- **SNR Levels**: 0 dB, 10 dB, 20 dB
- **Noise Types**: Factory, babble, environmental
- **Mixing**: Power-based SNR calculation
- **Strategies**:
  1. Baseline: Train clean, test clean+noisy
  2. Matched: Train clean+noisy, test separately

### Evaluation
- **Methodology**: Following lab4 (evaluate on full dataset)
- **Metrics**: Accuracy, Precision, Recall, F1-Score
- **Visualization**: Confusion matrices (20×20 heatmaps)
- **Support**: 40 samples per speaker (800 total)

---

## Results and Evaluation

### Performance Summary

**Task 3 - Baseline CNN (Clean Training)**:
- Clean Test: ~95-99% accuracy
- SNR 20 dB: ~79% accuracy
- SNR 10 dB: ~33% accuracy
- SNR 0 dB: ~16% accuracy

**Task 4 - Baseline Strategy (Clean Training, All Data Evaluation)**:
- Clean: ~91% accuracy
- SNR 20 dB: ~75% accuracy
- SNR 10 dB: ~39% accuracy
- SNR 0 dB: ~16% accuracy

**Task 4 - Matched Training (Clean + Noisy Training)**:
- Clean: ~100% accuracy
- SNR 20 dB: ~100% accuracy
- SNR 10 dB: ~100% accuracy
- SNR 0 dB: ~95% accuracy

### Key Findings

1. **Matched training significantly improves noise robustness**:
   - Near-perfect performance even at low SNR
   - Training on noisy data helps model learn noise-invariant features

2. **Clean-only training degrades rapidly with noise**:
   - Good performance on clean audio
   - Severe degradation below 10 dB SNR
   - Model overfits to clean acoustic conditions

3. **Confusion patterns**:
   - Similar-sounding names get confused (ahmed ↔ emad)
   - Longer names easier to recognize than short ones
   - Some speakers have more distinct voices

4. **Lab4 evaluation methodology**:
   - Evaluating on full dataset (800 samples) provides comprehensive view
   - Confusion matrices show 40 samples per speaker
   - Matches real-world testing scenarios better than small test sets

---

## Usage Guide

### Basic Workflow

**1. Initial Setup** (first time only):
```powershell
# Ensure Python 3.13 with required packages
pip install tensorflow numpy scipy soundfile librosa matplotlib seaborn scikit-learn tqdm

# Navigate to project
cd c:/Users/buhum/5lh/ss/TestingAOu-locase
```

**2. Run Complete Pipeline**:
```powershell
# Run all tasks in sequence
python run_project.py

# Expected output: All 6 tasks complete successfully
```

**3. View Results**:
```powershell
# Check confusion matrices
start results/task3/confusion_matrix.png
start results/task4/matched/confusion_clean_matched.png

# Read classification reports
type results/task4/matched/classification_report_clean_matched.txt
```

**4. Test with Your Voice**:
```powershell
# Record audio files in Audio_test/ folder
# Name format: speakername_###.wav (e.g., ahmed_000.wav)

# Test single file
python tasks/demo_test.py --audio Audio_test/ahmed_000.wav --true-label ahmed --top-n 10

# Test all recordings
python tasks/demo_test.py --pattern "Audio_test/*.wav" --true-label ahmed
```

### Advanced Usage

**Force Rerun Specific Task**:
```powershell
# Regenerate features
python tasks/task2.py

# Retrain baseline model
python tasks/task3.py

# Retrain noise compensation models
python tasks/task4_train.py
```

**Reset Everything**:
```powershell
# Delete all generated outputs
python reset_project.py

# Then rerun pipeline
python run_project.py
```

**Compare Models**:
```powershell
# Test with different models
python tasks/demo_test.py --pattern "Audio_test/*.wav" --true-label ahmed --model models/task3_baseline.keras
python tasks/demo_test.py --pattern "Audio_test/*.wav" --true-label ahmed --model models/task4_matched.keras
```

### Troubleshooting

**"No audio files found"**:
- Check Audio/ folder has 800 .wav files
- Verify file naming: speakername###.wav

**"Feature shape mismatch"**:
- Delete features/ folder and regenerate
- Run: `python tasks/task2.py`

**"Model not found"**:
- Run complete pipeline: `python run_project.py`
- Models should appear in models/ folder

**"Import errors"**:
- Install missing packages: `pip install <package>`
- Use Python 3.13 (project tested on this version)

**"Unicode errors in PowerShell"**:
- Normal on Windows (emoji characters)
- Functionality not affected

---

## File Descriptions Summary

### Core Scripts
| File | Purpose | Input | Output |
|------|---------|-------|--------|
| `run_project.py` | Master pipeline runner | - | Coordinates all tasks |
| `reset_project.py` | Clean all outputs | - | Deletes generated files |
| `tasks/task2.py` | Feature extraction | Audio/*.wav | features/features.npz |
| `tasks/task2_noisy.py` | Noisy feature extraction | Audio_noisy/*.wav | features/noisy_*.npz |
| `tasks/task3.py` | Baseline CNN training | features.npz | task3_baseline.keras |
| `tasks/task4_create_noise.py` | Generate noisy audio | Audio/, Audio_noise/ | Audio_noisy/ |
| `tasks/task4_train.py` | Noise compensation | All features | task4_*.keras |
| `tasks/task5.py` | Final evaluation | All models/features | Accuracy table |
| `tasks/demo_test.py` | Testing interface | Audio_test/*.wav | Predictions |

### Helper Modules
| File | Key Functions | Purpose |
|------|--------------|---------|
| `helpers/feature_utils.py` | MFCC extraction pipeline | Signal processing |
| `helpers/noise_utils.py` | SNR-based mixing | Noise augmentation |
| `helpers/check_speakers.py` | Dataset validation | Quality assurance |
| `helpers/find_duplicates.py` | Hash-based detection | Duplicate removal |
| `helpers/verify_features.py` | Shape/value checks | Feature validation |

### Documentation
| File | Content |
|------|---------|
| `TESTING_GUIDE.md` | How to test with new recordings |
| `PROJECT_DOCUMENTATION.md` | Complete technical documentation (this file) |
| `CRITICAL_FIXES.md` | Important bug fixes and changes |
| `docs/lab4.txt` | Original lab instructions |
| `docs/Project-instructions.txt` | Coursework specifications |
| `docs/README.md` | Quick start guide |

---

## Credits and References

**Course**: CMP-6026A/CMP-7016A – Audio-visual Processing  
**Institution**: [University Name]  
**Date**: November 2025

**Key Technologies**:
- Python 3.13
- TensorFlow/Keras (Deep Learning)
- NumPy (Numerical Computing)
- SciPy (Signal Processing)
- Librosa (Audio Analysis)
- Matplotlib/Seaborn (Visualization)
- Scikit-learn (Machine Learning Utilities)

**References**:
- Lab 4 Instructions (DNN with speech data)
- Project Specifications (Coursework assignment)
- MFCC Feature Extraction (Standard speech processing techniques)
- Noise Compensation Strategies (Matched training approach)

---

**End of Documentation**
